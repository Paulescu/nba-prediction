{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22180ec7-64a1-4341-bb0f-bf03a71f6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False  #set to True to use GPU for LightGBM \n",
    "OPTUNA = False #set to True to run Optuna first, false to use saved hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc0baa-cdf2-4062-91a2-c654f0974cb2",
   "metadata": {},
   "source": [
    "## LightGBM Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71787a4-84ae-44fc-bcaf-ea41b56807c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, \n",
    "    TimeSeriesSplit,\n",
    ")\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import (\n",
    "    early_stopping,\n",
    "    log_evaluation,\n",
    ")\n",
    "print('LGB version:', lgb.__version__)\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.lightgbm import (\n",
    "    NeptuneCallback, \n",
    "    create_booster_summary,\n",
    ")\n",
    "import neptune.new.integrations.optuna as optuna_utils\n",
    "from neptune.new.types import File\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shap\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from src.common_functions import plot_confusion_matrix\n",
    "\n",
    "from pathlib import Path  #for Windows/Linux compatibility\n",
    "DATAPATH = Path(r'data')\n",
    "\n",
    "import ipynbname\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9b487-cfb3-412f-8a2c-632981054ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME = \"train_selected.csv\"\n",
    "TEST_NAME = \"test_selected.csv\"\n",
    "\n",
    "train = pd.read_csv(DATAPATH / TRAIN_NAME)\n",
    "test = pd.read_csv(DATAPATH / TEST_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6ce3c-95d9-4545-8ab8-49fae4bcb29b",
   "metadata": {},
   "source": [
    "**Setup Neptuna.ai experiment tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5bf64-27eb-4727-926c-03148032b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_NOTE = \"\"\n",
    "\n",
    "try:\n",
    "    NEPTUNE_API_TOKEN = os.environ['NEPTUNE_API_TOKEN']\n",
    "except:\n",
    "    raise Exception('Set environment variable NEPTUNE_API_TOKEN')\n",
    "    \n",
    "PROJECT = \"cmunch1/nba-prediction\"\n",
    "PROJECT_OPTUNA = \"cmunch1/nba-optuna\" #for 2nd run if hyperparameters are tuned\n",
    "SOURCE = ipynbname.name()\n",
    "SOURCE_SPLIT = \"03_train_test_split.ipynb\"\n",
    "SOURCE_ENG = \"05_feature_engineering.ipynb\"\n",
    "SOURCE_SEL = \"06_feature_selection.ipynb\"\n",
    "    \n",
    "run = neptune.init(\n",
    "    project=PROJECT,\n",
    "    source_files=[SOURCE,SOURCE_SPLIT,SOURCE_ENG,SOURCE_SEL],\n",
    "    api_token=NEPTUNE_API_TOKEN,\n",
    ")\n",
    "neptune_callback = NeptuneCallback(run=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a328ca6-bf49-42a0-948d-4e1c3150c7dd",
   "metadata": {},
   "source": [
    "**Logging Note**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103b383-1d5c-414f-ba03-83849b266736",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"note\"] = LOGGING_NOTE\n",
    "run[\"sys/tags\"].add([\"lightgbm\",])\n",
    "run['dataset/train'] = TRAIN_NAME\n",
    "run['dataset/test'] = TEST_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d304655-f609-4512-b107-049cbeeb62a5",
   "metadata": {},
   "source": [
    "**Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508983c-17ad-4912-bf11-0916359d6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"model/parameters/GPU\"] = GPU\n",
    "run[\"model/parameters/OPTUNA\"] = OPTUNA\n",
    "\n",
    "if OPTUNA:\n",
    "    run[\"model/optuna/optuna_cv\"] = OPTUNA_CV = \"TimeSeriesSplit\"\n",
    "    #run[\"model/parameters/optuna_cv\"] = OPTUNA_CV = \"StratifiedKFold\"    \n",
    "    run[\"model/optuna/optuna_folds\"] = OPTUNA_FOLDS = 5\n",
    "    run[\"model/optuna/optuna_trials\"] = OPTUNA_TRIALS = 50\n",
    "\n",
    "run[\"model/parameters/k_folds\"] = K_FOLDS = 5\n",
    "run[\"model/parameters/seed\"] = SEED = 13\n",
    "#run[\"model/parameters/num_boost_round\"] = NUM_BOOST_ROUND = 700\n",
    "#run[\"model/parameters/enable_categorical\"] = ENABLE_CATEGORICAL = True\n",
    "run[\"model/parameters/early_stopping\"] = EARLY_STOPPING = 200 \n",
    "\n",
    "LOG_EVALUATION = 100\n",
    "VERBOSITY = 0\n",
    "\n",
    "STATIC_PARAMS = {\n",
    "                'seed': SEED,\n",
    "                'verbosity': -1,           \n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc', \n",
    "                }\n",
    "\n",
    "GPU_PARAMS = {\n",
    "            'device': 'gpu',\n",
    "            'gpu_platform_id': 0,\n",
    "            'gpu_device_id': 0,\n",
    "             }\n",
    "\n",
    "if GPU:\n",
    "    STATIC_PARAMS = STATIC_PARAMS | GPU_PARAMS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554abb2-d72b-45e9-b51d-f31f5aa8f3b6",
   "metadata": {},
   "source": [
    "**Fix Datatypes for smaller memory footprint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2db65-2803-45c0-831f-23423c610078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_datatypes(df):\n",
    "    df['GAME_DATE_EST'] = pd.to_datetime(df['GAME_DATE_EST'])\n",
    "\n",
    "    long_integer_fields = ['GAME_ID', 'HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON']\n",
    "\n",
    "    #convert long integer fields to int32 from int64\n",
    "    for field in long_integer_fields:\n",
    "        df[field] = df[field].astype('int32')\n",
    "    \n",
    "    #convert the remaining int64s to int8\n",
    "    for field in df.select_dtypes(include=['int64']).columns.tolist():\n",
    "        df[field] = df[field].astype('int8')\n",
    "        \n",
    "    #convert float64s to float16s\n",
    "    for field in df.select_dtypes(include=['float64']).columns.tolist():\n",
    "        df[field] = df[field].astype('float16')\n",
    "        \n",
    "    return df\n",
    "\n",
    "train = fix_datatypes(train)\n",
    "test = fix_datatypes(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b8472-7cb4-4c27-911d-9b444dffeea8",
   "metadata": {},
   "source": [
    "**Encode categoricals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489d8ef-fee8-4715-ab8e-93f855eb450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use special category feature capabalities in XGB and LGB, categoricals must be ints from 0 to N-1\n",
    "# Conversion can be accomplished by simple subtraction for several features\n",
    "# (these capabilities may or may not be used, but encoding does not hurt anything)\n",
    "\n",
    "def encode_categoricals(df):\n",
    "    first_team_ID = df['HOME_TEAM_ID'].min()\n",
    "    first_season = df['SEASON'].min()\n",
    "\n",
    " \n",
    "    df['HOME_TEAM_ID'] = (df['HOME_TEAM_ID'] - first_team_ID).astype('int8')\n",
    "    df['VISITOR_TEAM_ID'] = (df['VISITOR_TEAM_ID'] - first_team_ID).astype('int8')\n",
    "    df['SEASON'] = (df['SEASON'] - first_season).astype('int8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = encode_categoricals(train)\n",
    "test = encode_categoricals(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b91832-9691-4cf3-a203-c6804c32c3da",
   "metadata": {},
   "source": [
    "**Select Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9743c-82cf-44bc-a297-58d3ae6d10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['TARGET']\n",
    "test_target = test['TARGET']\n",
    "\n",
    "category_columns = ['HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'HOME_TEAM_WINS', 'PLAYOFF', 'CONFERENCE_x', 'CONFERENCE_y',]\n",
    "\n",
    "all_columns = train.columns.tolist()\n",
    "drop_columns = ['TARGET', 'GAME_DATE_EST', 'GAME_ID',] \n",
    "\n",
    "\n",
    "use_columns = [item for item in all_columns if item not in drop_columns]\n",
    "\n",
    "train = train[use_columns]\n",
    "test = test[use_columns]\n",
    "\n",
    "\n",
    "run[\"model/features\"].log(use_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56244a7-a900-4916-9268-7d5ae697c2d9",
   "metadata": {},
   "source": [
    "### Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66885d0c-86b1-4ef5-b5ab-cb42f25b3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    train_oof = np.zeros((train.shape[0],))\n",
    "    \n",
    "    \n",
    "    lgb_params= {\n",
    "                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "                \"learning_rate\": trial.suggest_loguniform('learning_rate', 1e-4, 0.5),\n",
    "                \"max_depth\": trial.suggest_categorical('max_depth', [5,10,20,40,100, -1]),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200000),\n",
    "                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 1000),\n",
    "                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 300),\n",
    "                \"cat_smooth\" : trial.suggest_int('min_data_per_groups', 1, 100)\n",
    "                }\n",
    "\n",
    "    lgb_params = lgb_params | STATIC_PARAMS\n",
    "        \n",
    "    #pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n",
    "    \n",
    "    if OPTUNA_CV == \"StratifiedKFold\": \n",
    "        kf = StratifiedKFold(n_splits=OPTUNA_FOLDS, shuffle=True, random_state=SEED)\n",
    "    elif OPTUNA_CV == \"TimeSeriesSplit\":\n",
    "        kf = TimeSeriesSplit(n_splits=OPTUNA_FOLDS)\n",
    "    \n",
    "\n",
    "    for f, (train_ind, val_ind) in (enumerate(kf.split(train, target))):\n",
    "\n",
    "        train_df, val_df = train.iloc[train_ind], train.iloc[val_ind]\n",
    "        \n",
    "        train_target, val_target = target[train_ind], target[val_ind]\n",
    "\n",
    "        train_lgbdataset = lgb.Dataset(train_df, label=train_target,categorical_feature=category_columns)\n",
    "        val_lgbdataset = lgb.Dataset(val_df, label=val_target, reference = train_lgbdataset, categorical_feature=category_columns)\n",
    "\n",
    "\n",
    "        model =  lgb.train(lgb_params, \n",
    "                           train_lgbdataset,\n",
    "                           valid_sets=val_lgbdataset,\n",
    "                           #num_boost_round = NUM_BOOST_ROUND,\n",
    "                           callbacks=[#log_evaluation(LOG_EVALUATION),\n",
    "                                      early_stopping(EARLY_STOPPING,verbose=False),\n",
    "                                      #pruning_callback,\n",
    "                                    ]               \n",
    "                           #verbose_eval= VERBOSE_EVAL,\n",
    "                          )\n",
    "\n",
    "        temp_oof = model.predict(val_df)\n",
    "\n",
    "        train_oof[val_ind] = temp_oof\n",
    "\n",
    "        #print(roc_auc_score(val_target, temp_oof))\n",
    "    \n",
    "    val_score = roc_auc_score(target, train_oof)\n",
    "    \n",
    "    return val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af54d6-05fe-4304-9897-56907ad7859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna():\n",
    "    \n",
    "     #log separate Neptune run for optuna hyperameter tuning\n",
    "    run2 = neptune.init(\n",
    "                    project=PROJECT_OPTUNA,\n",
    "                    source_files=[SOURCE,SOURCE_SPLIT,SOURCE_ENG,SOURCE_SEL],\n",
    "                    api_token=NEPTUNE_API_TOKEN,\n",
    "                    )\n",
    "    run2[\"options/optuna_cv\"] = OPTUNA_CV \n",
    "    run2[\"options/optuna_folds\"] = OPTUNA_FOLDS \n",
    "    run2[\"options/optuna_trials\"] = OPTUNA_TRIALS \n",
    "    run2[\"options/GPU\"] = GPU\n",
    "    #run2[\"options/enable_categorical\"] = ENABLE_CATEGORICAL\n",
    "    run2[\"features\"].log(use_columns)\n",
    "    run2[\"sys/tags\"].add([\"lightgbm\",])\n",
    "    \n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials = OPTUNA_TRIALS,)\n",
    "\n",
    "    optuna_utils.log_study_metadata(study, run2)\n",
    "    \n",
    "    print(\"Study Best Value:\",study.best_value)\n",
    "    print(\"Study Best Params:\",study.best_params)\n",
    "    \n",
    "    plot_optimization_history(study)\n",
    "    \n",
    "    plot_param_importances(study)\n",
    "    \n",
    "    run2[\"best_value\"] = study.best_value\n",
    "    run2[\"best_params\"] = study.best_params\n",
    "    run2[\"static_params\"] = STATIC_PARAMS\n",
    "    \n",
    "    run2.stop()\n",
    "    \n",
    "    return study.best_params\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c773dcf-9d6e-485c-935d-658a9e4bf489",
   "metadata": {},
   "source": [
    "**Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb917-63e5-4ff2-8b4a-d2fad52f199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTUNA:\n",
    "    tuned_params = run_optuna()\n",
    "else:\n",
    "    tuned_params = {\n",
    "                #current best hyperparameters from previous tuning:     \n",
    "                'lambda_l1': 1.795338637297326e-08, \n",
    "                'lambda_l2': 0.004705909102689521, \n",
    "                'learning_rate': 0.0514056673966814, \n",
    "                'max_depth': 100, \n",
    "                'n_estimators': 136307, \n",
    "                'feature_fraction': 0.707667657054092, \n",
    "                'bagging_fraction': 0.5528109129804049, \n",
    "                'bagging_freq': 1, \n",
    "                'num_leaves': 316, \n",
    "                'min_child_samples': 300, \n",
    "                'min_data_per_groups': 88,\n",
    "                }\n",
    "\n",
    "lgb_params= STATIC_PARAMS | tuned_params   \n",
    "\n",
    "run[\"model/params\"] = lgb_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32021550-606b-4a5a-833f-1d770097efce",
   "metadata": {},
   "source": [
    "**Setup Results table**\n",
    "\n",
    "Store key metrics for easy review later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9160d1-36c8-488a-8762-2e83eace0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame() #record metrics for easy comparison at the end\n",
    "\n",
    "#Load Simple Model results for later comparison\n",
    "def SimpleModel(test, true):\n",
    "    predict = test['HOME_W_PCT_x'].apply(lambda x: 0 if x < 0.50 else 1)\n",
    "    acc_score = accuracy_score(true, predict)\n",
    "    auc_score = roc_auc_score(true, predict)\n",
    "    \n",
    "    return acc_score , auc_score\n",
    "\n",
    "acc_score , auc_score = SimpleModel(test, test_target)\n",
    "df = {'Label': 'Simple Model', 'Accuracy': acc_score, 'AUC': auc_score, 'Threshold':\"N/A\"}\n",
    "results = results.append(df, ignore_index = True) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34042a79-5f67-4424-a2f3-4188d61c847b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a3b30-e182-42d3-ba2e-531189117223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(target,preds):\n",
    "    #for accuracy score, prediction probabilities must be convert to binary scores (Win or Lose)\n",
    "    #determine optimum threshold for conveting probablities using ROC curve\n",
    "    #generally 0.5 works for balanced data\n",
    "    #fpr = false positive rate, tpr = true postive rate\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target,preds)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_binary = (preds > optimal_threshold).astype(int)\n",
    "    \n",
    "    acc_score = accuracy_score(target, preds_binary)\n",
    "    auc_score = roc_auc_score(target, preds)\n",
    "\n",
    "    print()\n",
    "    print(\"Scores:\")\n",
    "    print()\n",
    "    print(\"Accuracy Score:\", acc_score)\n",
    "    print(\"AUC Score:\", auc_score)\n",
    "    print(\"Optimal Threshold:\", optimal_threshold)\n",
    "    \n",
    "    return preds_binary, acc_score, auc_score, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243dddf-9f3d-4104-94c3-c7255d296dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#initialize oof arrays including Shapley values \n",
    "train_oof = np.zeros((train.shape[0],))\n",
    "train_oof_shap = np.zeros((train.shape[0],train.shape[1]+1))\n",
    "#train_oof_shap_interact = np.zeros((train.shape[0],train.shape[1]+1,train.shape[1]+1))\n",
    "\n",
    "\n",
    "# K-fold cross validation\n",
    "\n",
    "kf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "for f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n",
    "    \n",
    "    train_df, val_df = train.iloc[train_ind], train.iloc[val_ind]\n",
    "    train_target, val_target = target[train_ind], target[val_ind]\n",
    "\n",
    "    train_lgbdataset = lgb.Dataset(train_df, label=train_target, categorical_feature=category_columns)\n",
    "    val_lgbdataset = lgb.Dataset(val_df, label=val_target, reference = train_lgbdataset, categorical_feature=category_columns)\n",
    "\n",
    "    model =  lgb.train(lgb_params, \n",
    "                       train_lgbdataset,\n",
    "                       valid_sets=val_lgbdataset,\n",
    "                       #num_boost_round = NUM_BOOST_ROUND,\n",
    "                       callbacks=[log_evaluation(LOG_EVALUATION),\n",
    "                                  early_stopping(EARLY_STOPPING,verbose=False),\n",
    "                                  neptune_callback],\n",
    "                       #verbose_eval= VERBOSE_EVAL,\n",
    "                      )\n",
    "\n",
    "    temp_oof = model.predict(val_df)\n",
    "    temp_oof_shap = model.predict(val_df, pred_contrib=True)\n",
    "    #temp_oof_shap_interact = model.predict(val_df, pred_interactions=True)\n",
    "\n",
    "    train_oof[val_ind] = temp_oof\n",
    "\n",
    "    train_oof_shap[val_ind, :] = temp_oof_shap\n",
    "    #train_oof_shap_interact[val_ind, :,:] = temp_oof_shap_interact\n",
    "    \n",
    "    temp_oof_binary, acc_score, auc_score, optimal_threshold = get_scores(val_target, temp_oof)\n",
    "\n",
    "    \n",
    "# Out-of-Fold composite for train data\n",
    "\n",
    "train_oof_binary, acc_score, auc_score, optimal_threshold = get_scores(target,train_oof)\n",
    "\n",
    "#neptune.ai logging    \n",
    "\n",
    "run[\"train/accuracy\"] = acc_score = accuracy_score(target, train_oof_binary)\n",
    "run[\"train/AUC\"] = auc_score = roc_auc_score(target, train_oof)\n",
    "run[\"train/optimal_threshold\"] = optimal_threshold\n",
    "                                                          \n",
    "df = {'Label': 'Train', 'Accuracy': acc_score, 'AUC': auc_score, 'Threshold':optimal_threshold}\n",
    "results = results.append(df, ignore_index = True)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22c10d-a994-4baa-a138-1459471567dc",
   "metadata": {},
   "source": [
    "**OOF Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbea0bc-5e88-44c2-833d-1138f38ef6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(target, train_oof_binary)\n",
    "print(cm)\n",
    "fig = plot_confusion_matrix(cm,['win','lose'])\n",
    "run[\"train/confusion_matrix\"].upload(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d910991-3dd2-4927-81a9-107a2ac04f34",
   "metadata": {},
   "source": [
    "**OOF Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5856a9c-b650-42e3-a771-138e1c78baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"train/classification_report\"] = classification_report(target, train_oof_binary)\n",
    "print(classification_report(target, train_oof_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f3f39-087c-4743-991b-11ebb4776be3",
   "metadata": {},
   "source": [
    "**Train Feature Importance via Split - the number of times a feature is used in the model**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d77e632-e98f-4a69-bf3a-bbf20fb1c96b",
   "metadata": {},
   "source": [
    "max_features = 25\n",
    "max_title = 'Top ' + str(max_features) + ' Feature importance - Splits'\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "lgb.plot_importance(model, importance_type='split', max_num_features=max_features, title=max_title ,ax=ax)\n",
    "run[\"train/feature_importance_split\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ce051-7ccb-49f7-b666-564199579e3f",
   "metadata": {},
   "source": [
    "**Train Feature Importance via Gain - the average gain of splits which use the feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70612253-a661-40ca-8584-4de93c41e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 25\n",
    "max_title = 'Top ' + str(max_features) + ' Feature importance - Gain'\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "lgb.plot_importance(model, importance_type='gain', max_num_features=max_features, title=max_title ,ax=ax)\n",
    "run[\"train/feature_importance_gain\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fac593-df38-459f-b7d1-48a06a30258f",
   "metadata": {},
   "source": [
    "**OOF Feature Importance via Shapley values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aded9c7-d089-4daf-9966-4c6eba368cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "shap.summary_plot(train_oof_shap[:,:-1], train)\n",
    "run[\"train/shapley_summary\"].upload(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d737a84-793e-4a19-b36b-88462f4209ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "shap.summary_plot(train_oof_shap[:,:-1], train[use_columns], plot_type=\"bar\")\n",
    "run[\"train/shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20da39-e086-4b2a-bc29-383a6bc7ff76",
   "metadata": {},
   "source": [
    "### Test Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ea911-966d-49a2-acec-a4763bd06d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_preds = model.predict(test)\n",
    "test_preds_shap = model.predict(test, pred_contrib=True)\n",
    "\n",
    "test_preds_binary, acc_score, auc_score, optimal_threshold = get_scores(test_target, test_preds)\n",
    "\n",
    "run[\"test/accuracy\"] = acc_score = accuracy_score(test_target, test_preds_binary)\n",
    "run[\"test/AUC\"] = auc_score = roc_auc_score(test_target, test_preds)\n",
    "run[\"test/optimal_threshold\"] = optimal_threshold\n",
    "\n",
    "df = {'Label': 'Test', 'Accuracy': acc_score, 'AUC': auc_score, 'Threshold':optimal_threshold}\n",
    "results = results.append(df, ignore_index = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc7035-973a-4008-bb60-0d34afcb25e9",
   "metadata": {},
   "source": [
    "**Test Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c69ef4-60c5-4f42-a38f-03cfae9ba941",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_target, test_preds_binary)\n",
    "print(cm)\n",
    "fig = plot_confusion_matrix(cm,['win','lose'])\n",
    "run[\"test/confusion_matrix\"].upload(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e53c0-a375-4e37-a81e-c501eeac83ca",
   "metadata": {},
   "source": [
    "**Test Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f58b4-12ee-4997-b158-bf8d5b8d22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"test/classification_report\"] = classification_report(test_target, test_preds_binary)\n",
    "print(classification_report(test_target, test_preds_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97fb0e-2af4-4d77-a921-83bcdfa5205a",
   "metadata": {},
   "source": [
    "**Test Feature Importance via Shapley values**\n",
    "\n",
    "For comparison to cross-validation OOF Shapley values to ensure that the model is working in similar manner on the test data as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf8e5b-6b9a-4b06-a325-d0d40e06e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "shap.summary_plot(test_preds_shap[:,:-1], test)\n",
    "run[\"test/shapley_summary\"].upload(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d4fcf-d2a9-4a8e-bd47-f59894744503",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "shap.summary_plot(test_preds_shap[:,:-1], test[use_columns], plot_type=\"bar\")\n",
    "run[\"test/shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91bd57-1434-485a-8119-5066dc7d2ede",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    " - Compare Simple model predictions vs ML Test data predictions\n",
    " - Compare OOF/Train data vs Test/Validation data\n",
    " - Compare early season Test data vs later season Test data\n",
    " \n",
    " Feature importances via Shapley values are *local* to the given dataset and can assist in adversarial validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54cb8a-0e3c-4713-aaa2-8fde10ff4b6c",
   "metadata": {},
   "source": [
    "**Split Test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8dd7e-57cb-407e-800b-e0304d261cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATAPATH / TEST_NAME)\n",
    "test = fix_datatypes(test)\n",
    "test = encode_categoricals(test)\n",
    "\n",
    "SPLIT = pd.to_datetime(\"2022-01-01\")\n",
    "\n",
    "run[\"test_split_1/end_date\"] = SPLIT\n",
    "run[\"test_split_2/start_date\"] = SPLIT\n",
    "\n",
    "test1 = test[test['GAME_DATE_EST'] < SPLIT]\n",
    "test2 = test[test['GAME_DATE_EST'] >= SPLIT]\n",
    "\n",
    "test1_target = test1['TARGET']\n",
    "test2_target = test2['TARGET']\n",
    "\n",
    "test1 = test1[use_columns]\n",
    "test2 = test2[use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472e30-0eb3-4ad5-846e-449201c71927",
   "metadata": {},
   "source": [
    "**Early season results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0c9d8-e1a9-40a3-b617-cedd6778bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_splits(label, test, test_target, results):\n",
    "\n",
    "    test_preds = model.predict(test)\n",
    "    test_preds_shap = model.predict(test, pred_contrib=True)\n",
    "\n",
    "    test_preds_binary, acc_score, auc_score, optimal_threshold = get_scores(test_target, test_preds)\n",
    "\n",
    "    run[\"test_split_\" + label + \"/accuracy\"] = acc_score \n",
    "    run[\"test_split_\" + label + \"/AUC\"] = auc_score \n",
    "    run[\"test_split_\" + label + \"/optimal_threshold\"] = optimal_threshold\n",
    "\n",
    "    df = {'Label': label, 'Accuracy': acc_score, 'AUC': auc_score, 'Threshold':optimal_threshold}\n",
    "    results = results.append(df, ignore_index = True) \n",
    "\n",
    "    run[\"test_split_\" + label + \"/classification_report\"] = classification_report(test_target, test_preds_binary)\n",
    "    print(classification_report(test_target, test_preds_binary))\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "    shap.summary_plot(test_preds_shap[:,:-1], test, plot_type=\"bar\")\n",
    "    run[\"test_split_\" + label + \"/shapley_summary_bar\"].upload(fig)\n",
    "\n",
    "    #Simple model applied to split\n",
    "    acc_score, auc_score = SimpleModel(test, test_target)\n",
    "    df = {'Label': 'Simple-' + label, 'Accuracy': acc_score, 'AUC': auc_score, 'Threshold':\"N/A\"}\n",
    "    results = results.append(df, ignore_index = True) \n",
    "    \n",
    "    return test_preds_shap, results\n",
    "    \n",
    "print(\"TEST1\")\n",
    "test_preds_shap1, results = process_splits('Test1',test1, test1_target, results)\n",
    "print(\"TEST2\")\n",
    "test_preds_shap2, results = process_splits('Test2',test2, test2_target, results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35206b-318a-4c9e-801f-5925718873cd",
   "metadata": {},
   "source": [
    "**Summary Table**\n",
    "\n",
    "Key metrics from Simple Model, Train, Test, and Test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3265fcd-ab37-4d27-bb5e-eca02fa550d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"evaluation/summary_table\"].upload(File.as_html(results))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57650290-e942-47f2-a06b-b19b284bc566",
   "metadata": {},
   "source": [
    "**Train vs Test Feature Importances via Shapley Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c72ff9-c022-4a6a-b2d8-1e7ca031763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "shap.summary_plot(train_oof_shap[:,:-1], train[use_columns], plot_type=\"bar\", plot_size=None, show=False)\n",
    "plt.subplot(1,2,2)\n",
    "shap.summary_plot(test_preds_shap[:,:-1], test1[use_columns], plot_type=\"bar\", plot_size=None, show=False)\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "run[\"evaluation/test_train_shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdf354-3c4a-4660-8c96-68b7113d40ec",
   "metadata": {},
   "source": [
    "**Test1 vs Test2 Feature Importances via Shapley Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54981690-7b97-46d9-8157-262fc033e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "shap.summary_plot(test_preds_shap1[:,:-1], test1[use_columns], plot_type=\"bar\", plot_size=None, show=False)\n",
    "plt.subplot(1,2,2)\n",
    "shap.summary_plot(test_preds_shap2[:,:-1], test1[use_columns], plot_type=\"bar\", plot_size=None, show=False)\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "run[\"evaluation/test1_test2_shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35207419-7d26-4bd1-800b-f9969e2d9ad4",
   "metadata": {},
   "source": [
    "**End experiment tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469ee75-dc3f-4951-88cf-5f9987a71391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end experiment tracking\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba",
   "language": "python",
   "name": "nba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
